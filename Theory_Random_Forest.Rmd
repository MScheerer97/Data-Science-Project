---
title: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
body {
text-align: justify;
background-color:black;
color:white;}
</style>

\

**Random Forest** is a robust machine learning algorithm that can be used for a variety of tasks including regression and classification. It is an ensemble method, meaning that a random forest model is made up of a large number of **small decision trees**, called estimators, which each produce their own predictions. The random forest model combines the predictions of the estimators to produce a more accurate prediction.

In order to understand Random Forests, it is necessary to understand how **decision trees** work: \

**Decision Trees explained**: \

A decision tree is a simple way of classifying examples.

1. Step: Choose the attribute which can most effectively split the dataset into different classes \

2. Step: Split the dataset on this attribute. This corresponds to creating a new node in the decision tree \

3. Step: For each split of the dataset, repeat the process of splitting the dataset on the best attribute \

4. Step: Stop creating new nodes in the tree if all the samples in the current node already belong to the same class, or if no feature provides any value, or if the tree has already reached its maximum allowed depth \

<p align="center">
  <img width="500" src="https://gdcoder.com/content/images/2019/05/Screen-Shot-2019-05-17-at-00.09.26.png"/>
</p>

<p style="text-align: center;">(Source: https://gdcoder.com/content/images/2019/05/Screen-Shot-2019-05-17-at-00.09.26.png)</p> 
<br>


**Decision Trees Disadvantages**

Decision trees are useful models because they allow a human to instantly visualize the decision-making process. 
The major problem is that decision trees that are grown very deep tend to learn patterns that are very particular to the training dataset.

This means that a single decision tree is likely to be very strongly adapted to its training set and generalize poorly to unseen examples. In other words, the decision tree overfits the training set.

Random Forests were introduced as a modification to the basic decision tree algorithms which makes them more robust and corrects for the problem of overfitting.


**Random Forest algorithm explained**: \

There are a number of variants of the random forest algorithm, but the most widely used version in use today is based on Leo Breiman's 2001 paper.

There is a training set of *N* training examples, and for each example we have *N* features. A Random Forest will consist
of *N tree* decision trees.

1. Step: **Bagging** \
* Subset *n* examples of all N examples, whereby n < N. Sampling is done at random but with replacement. This subsampling
process is called *bootstrap aggregation*, or *bagging*.

2. Step: **Random subspace method** \
* Each row or training example has *M* features, whereby only a part of a total of *M* features will be taken. Each estimator sees
only m < M features. As as consequence, no  estimator will see the full training set, but only *m* features with *n* training examples

3. Step: **Training estimators** \
* *N tree* decision trees are created , or estimators, and trained each one on a different set of m features and n training examples. The trees are not pruned, as they would be in the case of training a simple decision tree classifier.

4. Step: **Perform inference by aggregating predictions of estimators** \
* To make a prediction for a new incoming example, we pass the relevant features of this example to each of the *N tree* estimators. We will obtain *N tree* predictions, which we need to combine to produce the overall prediction of the random forest. In the case of regression the mean value of the predictors of all estimators is used. \

This use of many estimators is the reason why the random forest algorithm is called an ensemble method. Each individual estimator is a weak learner, but when many weak estimators are combined together they can produce a much stronger learner. Ensemble methods take a 'strength in numbers' approach, where the output of many small models is combined to produce a much more accurate and powerful prediction.

<p align="center">
  <img width="500" src="https://miro.medium.com/max/1400/1*ZFuMI_HrI3jt2Wlay73IUQ.png"/>
</p>

<p style="text-align: center;">(Source: https://levelup.gitconnected.com/random-forest-regression-209c0f354c84)</p> 
<br>

**Random Forest Hyperparameters:** \

There are mainly 3 hyperparameters to tune:

1. *N tree* Number of trees that the algorithm builds before averaging the predictions

2. *Max features m* denote the maximum number of features that random forests considers splitting a node

3. *Minimum sample lead* determines the minimum number of leaves required to split an internal node 

**Random Forest Advantages**: \

* Accurate prediction  and better generalizations due to utilization of ensemble strategies and random sampling \

* In general, very good performance with regression and classification tasks \

* More robust to overfitting than standard decision trees classifiers and many other Machine Learning algorithms \

* Good at handling large datasets, high dimensionality and heterogeneous feature types and missing values \

* Very few and "easy to understand" hyperparameters \

* In many cases, the baseline hyperparameters perform very good, so tuning becomes less important than with other algorithms \

* In training data, they are less sensitive to outlier data \

* Variable importance and accuracy is generated automatically \

* In comparison to decision trees, there is no need to prune the trees \

* One of the first algorithms for Data Scientists when developing a new machine learning system \

**Random Forest Disadvantages**: \

* In contrast to other algorithms, Random Forests are black boxes and do not have a straightforward interpretation \

* Slow to train and run, and they produce large file sizes \

*Sources and for further information*

* Breiman, L. (2001). Random Forests. Machine Learning. 45. 5-32. 10.1023/A:1010950718922. 
* Ali, Jehad & Khan, Rehanullah & Ahmad, Nasir & Maqsood, Imran. (2012). Random Forests and Decision Trees. International Journal of Computer Science Issues(IJCSI). 9. 
* Random forests. DeepAI. (2020, September 10). Retrieved February 9, 2022, from https://deepai.org/machine-learning-glossary-and-terms/random-forest 
* Random Forest: Introduction to random forest algorithm. Analytics Vidhya. (2021, June 24). Retrieved February 9, 2022, from https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/ 
* Yiu, T. (2021, September 29). Understanding random forest. Medium. Retrieved February 9, 2022, from https://towardsdatascience.com/understanding-random-forest-58381e0602d2#:~:text=The%20random%20forest%20is%20a,that%20of%20any%20individual%20tree. 


